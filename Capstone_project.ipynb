{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e068bf-dd83-43ba-8dac-188a28273ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseValue  \n",
       "0    -122.23          4.526  \n",
       "1    -122.22          3.585  \n",
       "2    -122.24          3.521  \n",
       "3    -122.25          3.413  \n",
       "4    -122.25          3.422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Categorical Features: []\n",
      "Baseline Model - MSE: 1.3762028164626938 MAE: 0.8740399224806202 R2: -0.050208628996205595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.271148</td>\n",
       "      <td>0.344458</td>\n",
       "      <td>0.793081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.298926</td>\n",
       "      <td>0.377162</td>\n",
       "      <td>0.771884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.467507</td>\n",
       "      <td>0.504830</td>\n",
       "      <td>0.643236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.467509</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.643235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.528135</td>\n",
       "      <td>0.477494</td>\n",
       "      <td>0.596969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN Regressor</td>\n",
       "      <td>0.674144</td>\n",
       "      <td>0.619152</td>\n",
       "      <td>0.485547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model       MSE       MAE        R2\n",
       "3      Random Forest  0.271148  0.344458  0.793081\n",
       "4  Gradient Boosting  0.298926  0.377162  0.771884\n",
       "1   Ridge Regression  0.467507  0.504830  0.643236\n",
       "0  Linear Regression  0.467509  0.505060  0.643235\n",
       "2      Decision Tree  0.528135  0.477494  0.596969\n",
       "5      KNN Regressor  0.674144  0.619152  0.485547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "\n",
      "‚úÖ Broad Randomized Search Complete\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}\n",
      "Best CV R¬≤: 0.7985705307275951\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "‚úÖ Narrow Grid Search Complete\n",
      "Best Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best CV R¬≤: 0.7985705307275951\n",
      "\n",
      "üèÜ Final Tuned Random Forest Performance\n",
      "---------------------------------------\n",
      "R¬≤ Score (Test): 0.7923\n",
      "MAE (Test): 0.3514\n",
      "MSE (Test): 0.2722\n",
      "\n",
      "Total Training Time: 1.07 min\n",
      "\n",
      "‚öôÔ∏è Training SGD Regressor...\n",
      "‚úÖ SGD R¬≤: -72906.2561\n",
      "MAE: 245.0493\n",
      "MSE: 95538.3229\n",
      "\n",
      "‚öôÔ∏è Training baseline SVR (RBF kernel) on 15% data for efficiency...\n",
      "‚úÖ SVR (Baseline, subsampled) R¬≤: 0.3821\n",
      "MAE: 0.6584\n",
      "MSE: 0.7549\n",
      "\n",
      "‚öôÔ∏è RandomizedSearchCV: Optimizing SVR Hyperparameters...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "‚úÖ SVR Randomized Search Complete\n",
      "Best Parameters: {'gamma': 0.01, 'epsilon': 0.2, 'C': np.float64(31.622776601683793)}\n",
      "Best CV R¬≤: 0.6178\n",
      "üèÜ Tuned SVR R¬≤ (test subset): 0.6484\n",
      "\n",
      "üìà Generating learning curves (Bias‚ÄìVariance Analysis)...\n",
      "\n",
      "üìä Visualizing SVR Margin (Epsilon-Tube)...\n",
      "\n",
      "üìä Generating full model comparison with professional visuals...\n",
      "üìä Full Model Comparison Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (Tuned)</td>\n",
       "      <td>0.272186</td>\n",
       "      <td>0.351409</td>\n",
       "      <td>0.792290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.298926</td>\n",
       "      <td>0.377162</td>\n",
       "      <td>0.771884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>0.467507</td>\n",
       "      <td>0.504830</td>\n",
       "      <td>0.643236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.467509</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.643235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned SVR</td>\n",
       "      <td>0.484519</td>\n",
       "      <td>0.500640</td>\n",
       "      <td>0.630254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.528135</td>\n",
       "      <td>0.477494</td>\n",
       "      <td>0.596969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN Regressor</td>\n",
       "      <td>0.674144</td>\n",
       "      <td>0.619152</td>\n",
       "      <td>0.485547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGD Regressor</td>\n",
       "      <td>95538.322926</td>\n",
       "      <td>245.049300</td>\n",
       "      <td>-72906.256065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model           MSE         MAE            R2\n",
       "0  Random Forest (Tuned)      0.272186    0.351409      0.792290\n",
       "1      Gradient Boosting      0.298926    0.377162      0.771884\n",
       "2       Ridge Regression      0.467507    0.504830      0.643236\n",
       "3      Linear Regression      0.467509    0.505060      0.643235\n",
       "4              Tuned SVR      0.484519    0.500640      0.630254\n",
       "5          Decision Tree      0.528135    0.477494      0.596969\n",
       "6          KNN Regressor      0.674144    0.619152      0.485547\n",
       "7          SGD Regressor  95538.322926  245.049300 -72906.256065"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è SGD Regressor is extremely unstable; metrics are on a very different scale:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD Regressor</td>\n",
       "      <td>95538.322926</td>\n",
       "      <td>245.0493</td>\n",
       "      <td>-72906.256065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model           MSE       MAE            R2\n",
       "0  SGD Regressor  95538.322926  245.0493 -72906.256065"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Dynamic Summary & Recommendation:\n",
      "- The top-performing model based on R¬≤ is **Random Forest (Tuned)** (R¬≤ = 0.792).\n",
      "- Lowest MAE: **Random Forest (Tuned)** (MAE = 0.351), indicating best average prediction accuracy.\n",
      "- Lowest MSE: **Random Forest (Tuned)** (MSE = 0.272), indicating best overall error minimization.\n",
      "- Random Forest and Gradient Boosting are strong classical models; Tuned SVR provides a competitive non-linear alternative.\n",
      "- SGD Regressor is not recommended due to extreme instability and poor performance.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Housing Price Prediction Capstone Project\n",
    "# =========================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # Housing Price Prediction Capstone Project\n",
    "# **Author:** Your Name  \n",
    "# **Date:** 2025-10-24\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ## Summary\n",
    "# End-to-end regression modeling for housing prices: data preprocessing, feature engineering, EDA, baseline & multiple models, hyperparameter tuning, SVR with kernel trick, bias-variance analysis, and final evaluation visualizations.\n",
    "\n",
    "# %% [code]\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from math import pi\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Inline plots\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load and Inspect Dataset\n",
    "\n",
    "# %% [code]\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Convert to DataFrame\n",
    "housing_df = pd.DataFrame(housing.data,\n",
    "                             columns=housing.feature_names)\n",
    "housing_df['MedHouseValue'] = pd.Series(housing.target)\n",
    "display(housing_df.head())\n",
    "#print(housing_df.info())\n",
    "#print(housing_df.describe())\n",
    "#print(housing_df.isnull().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Feature & Task Understanding\n",
    "\n",
    "# %% [code]\n",
    "target = 'MedHouseValue'\n",
    "predictors = [col for col in housing_df.columns if col != target]\n",
    "numerical_features = housing_df[predictors].select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = housing_df[predictors].select_dtypes(include=['object','category']).columns.tolist()\n",
    "print(\"Numerical Features:\", numerical_features)\n",
    "print(\"Categorical Features:\", categorical_features)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(housing_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.savefig(\"images/Correlation Heatmap.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Preprocessing\n",
    "\n",
    "# %% [code]\n",
    "X = housing_df[predictors]\n",
    "y = housing_df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "# Encode categorical features\n",
    "if categorical_features:\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "    X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[categorical_features]),\n",
    "                                   columns=encoder.get_feature_names_out(categorical_features),\n",
    "                                   index=X_train.index)\n",
    "    X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_features]),\n",
    "                                  columns=encoder.get_feature_names_out(categorical_features),\n",
    "                                  index=X_test.index)\n",
    "    X_train = X_train.drop(categorical_features, axis=1).join(X_train_encoded)\n",
    "    X_test = X_test.drop(categorical_features, axis=1).join(X_test_encoded)\n",
    "    \n",
    "# %% [markdown]\n",
    "# ## Feature Engineering\n",
    "\n",
    "# %% [code]\n",
    "X_train['Rooms_per_Household'] = X_train['AveRooms'] / X_train['AveOccup']\n",
    "X_test['Rooms_per_Household'] = X_test['AveRooms'] / X_test['AveOccup']\n",
    "X_train['Bedrooms_per_Room'] = X_train['AveBedrms'] / X_train['AveRooms']\n",
    "X_test['Bedrooms_per_Room'] = X_test['AveBedrms'] / X_test['AveRooms']\n",
    "X_train['Population_per_Household'] = X_train['Population'] / X_train['AveOccup']\n",
    "X_test['Population_per_Household'] = X_test['Population'] / X_test['AveOccup']\n",
    "#all_features = X_train.columns.tolist()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6) Exploratory Data Analysis (EDA)\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# %% [code]\n",
    "# Histograms\n",
    "housing_df['Rooms_per_Household'] = housing_df['AveRooms'] / housing_df['AveOccup']\n",
    "housing_df['Bedrooms_per_Room'] = housing_df['AveBedrms'] / housing_df['AveRooms']\n",
    "housing_df['Population_per_Household'] = housing_df['Population'] / housing_df['AveOccup']\n",
    "\n",
    "derived_features = ['Rooms_per_Household', 'Bedrooms_per_Room', 'Population_per_Household']\n",
    "all_features = numerical_features + derived_features\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(all_features):\n",
    "    sns.histplot(X_train[feature], bins=30, kde=True, ax=axes[i], color=palette[i % len(palette)])\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "# Remove the last empty subplot (if features < 12)\n",
    "if len(all_features) < len(axes):\n",
    "    for j in range(len(all_features), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"housing_feature_histograms.png\", dpi=300, bbox_inches='tight')\n",
    "plt.savefig(\"images/housing_feature_histograms.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "# Scatter plots\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=X_train['MedInc'], y=y_train, alpha=0.5, color='royalblue')\n",
    "plt.title(\"Median Income vs Median House Value\")\n",
    "plt.xlabel(\"Median Income (scaled)\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/median_income_vs_housevalue.png\")\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=X_train['Longitude'], y=X_train['Latitude'], hue=y_train, palette='viridis', alpha=0.5)\n",
    "plt.title(\"Geographic Distribution of House Values\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.legend(title=\"MedHouseValue\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/geographic_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "#plt.show()\n",
    "plt.close()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Outlier Handling\n",
    "\n",
    "# %% [code]\n",
    "def cap_outliers(df, features):\n",
    "    capped = df.copy()\n",
    "    for feature in features:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5*IQR\n",
    "        upper = Q3 + 1.5*IQR\n",
    "        capped[feature] = np.where(capped[feature] < lower, lower, capped[feature])\n",
    "        capped[feature] = np.where(capped[feature] > upper, upper, capped[feature])\n",
    "    return capped\n",
    "\n",
    "X_train_capped = cap_outliers(X_train, all_features)\n",
    "X_test_capped = cap_outliers(X_test, all_features)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Baseline Model\n",
    "\n",
    "# %% [code]\n",
    "baseline_pred = np.median(y_train)\n",
    "baseline_mse = mean_squared_error(y_test, [baseline_pred]*len(y_test))\n",
    "baseline_mae = mean_absolute_error(y_test, [baseline_pred]*len(y_test))\n",
    "baseline_r2 = r2_score(y_test, [baseline_pred]*len(y_test))\n",
    "print(\"Baseline Model - MSE:\", baseline_mse, \"MAE:\", baseline_mae, \"R2:\", baseline_r2)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9) Regression Model Comparison (Existing Models)\n",
    "\n",
    "# %% [code]\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"KNN Regressor\": KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_capped, y_train)\n",
    "    y_pred = model.predict(X_test_capped)\n",
    "    results.append([name,\n",
    "                    mean_squared_error(y_test, y_pred),\n",
    "                    mean_absolute_error(y_test, y_pred),\n",
    "                    r2_score(y_test, y_pred)])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model','MSE','MAE','R2'])\n",
    "results_df.sort_values('R2', ascending=False, inplace=True)\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Random Forest Regressor ‚Äì Hyperparameter Optimization (RandomizedSearchCV)\n",
    "\n",
    "# RandomForest is a robust ensemble method that averages multiple decision trees to reduce overfitting.  \n",
    "# However, exhaustive `GridSearchCV` on large datasets can be very time-consuming.  \n",
    "# To optimize runtime without compromising accuracy, we switch to **`RandomizedSearchCV`**, which samples\n",
    "# from a defined parameter space to find strong hyperparameters efficiently.\n",
    "\n",
    "# %% [code]\n",
    "# \n",
    "# Track runtime\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------------------\n",
    "# Stage 1: Broad Randomized Search\n",
    "# ----------------------------\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=6,                   \n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_rf.fit(X_train_capped, y_train)\n",
    "best_params_stage1 = random_search_rf.best_params_\n",
    "\n",
    "print(\"\\n‚úÖ Broad Randomized Search Complete\")\n",
    "print(\"Best Parameters:\", best_params_stage1)\n",
    "print(\"Best CV R¬≤:\", random_search_rf.best_score_)\n",
    "\n",
    "# ----------------------------\n",
    "# Stage 2: Narrow Grid Search\n",
    "# ----------------------------\n",
    "# Only vary around best results ‚Äî keep combinations <= 20\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [best_params_stage1['n_estimators']],\n",
    "    'max_depth': [best_params_stage1['max_depth'], None],\n",
    "    'min_samples_split': [best_params_stage1['min_samples_split']],\n",
    "    'min_samples_leaf': [best_params_stage1['min_samples_leaf']],\n",
    "    'max_features': [best_params_stage1['max_features']]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_capped, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(\"\\n‚úÖ Narrow Grid Search Complete\")\n",
    "print(\"Best Parameters:\", grid_search_rf.best_params_)\n",
    "print(\"Best CV R¬≤:\", grid_search_rf.best_score_)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Final Tuned Model\n",
    "# ----------------------------\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred_best_rf = best_rf.predict(X_test_capped)\n",
    "print(\"\\nüèÜ Final Tuned Random Forest Performance\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"R¬≤ Score (Test): {r2_score(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"MAE (Test): {mean_absolute_error(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"MSE (Test): {mean_squared_error(y_test, y_pred_best_rf):.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Training Time: {(time.time() - start_time)/60:.2f} min\")\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization 1: Actual vs Predicted\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_best_rf, alpha=0.6, color='royalblue')\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Median House Value\")\n",
    "plt.ylabel(\"Predicted Median House Value\")\n",
    "plt.title(\"Final Random Forest (Optimized) - Actual vs Predicted Prices\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/Random Forest (Optimized) - Actual vs Predicted Prices.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization 2: Feature Importances\n",
    "# ----------------------------\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\n",
    "        'Feature': X_train_capped.columns,\n",
    "        'Importance': best_rf.feature_importances_\n",
    "    })\n",
    "    .sort_values('Importance', ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', hue='Feature',data=feat_imp.head(15), palette='bright')\n",
    "plt.title(\"Top 15 Feature Importances ‚Äì Final Random Forest\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/Top 15 Feature Importances.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ##  Additional Models + Optimized SVR Tuning + Bias‚ÄìVariance Analysis\n",
    "# Includes: SGDRegressor, SVR with kernel trick, randomized hyper-parameter search,\n",
    "# bias‚Äìvariance learning curves, and epsilon-tube visualization.\n",
    "\n",
    "# %% [code]\n",
    "# # ------------------------------------------------------\n",
    "# SGD Regressor (Baseline)\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n‚öôÔ∏è Training SGD Regressor...\")\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd.fit(X_train_capped, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test_capped)\n",
    "\n",
    "print(f\"‚úÖ SGD R¬≤: {r2_score(y_test, y_pred_sgd):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred_sgd):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_sgd):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Baseline SVR (with subsample to reduce runtime)\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n‚öôÔ∏è Training baseline SVR (RBF kernel) on 15% data for efficiency...\")\n",
    "\n",
    "sample_frac = 0.15\n",
    "X_train_svr = X_train_capped.sample(frac=sample_frac, random_state=42)\n",
    "y_train_svr = y_train.loc[X_train_svr.index]\n",
    "X_test_svr = X_test_capped.sample(frac=sample_frac, random_state=42)\n",
    "y_test_svr = y_test.loc[X_test_svr.index]\n",
    "\n",
    "svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "svr.fit(X_train_svr, y_train_svr)\n",
    "y_pred_svr = svr.predict(X_test_svr)\n",
    "print(f\"‚úÖ SVR (Baseline, subsampled) R¬≤: {r2_score(y_test_svr, y_pred_svr):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test_svr, y_pred_svr):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test_svr, y_pred_svr):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# RandomizedSearchCV for SVR Hyperparameter Optimization\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n‚öôÔ∏è RandomizedSearchCV: Optimizing SVR Hyperparameters...\")\n",
    "\n",
    "param_dist_svr = {\n",
    "    'C': np.logspace(1, 3, 5),         # [10, 31.6, 100, 316, 1000]\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.2],\n",
    "    'epsilon': [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(\n",
    "    SVR(kernel='rbf'),\n",
    "    param_distributions=param_dist_svr,\n",
    "    n_iter=8,                # small and fast\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_svr.fit(X_train_svr, y_train_svr)\n",
    "best_svr = random_search_svr.best_estimator_\n",
    "print(\"\\n‚úÖ SVR Randomized Search Complete\")\n",
    "print(\"Best Parameters:\", random_search_svr.best_params_)\n",
    "print(f\"Best CV R¬≤: {random_search_svr.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned SVR\n",
    "y_pred_best_svr = best_svr.predict(X_test_svr)\n",
    "print(f\"üèÜ Tuned SVR R¬≤ (test subset): {r2_score(y_test_svr, y_pred_best_svr):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Bias‚ÄìVariance (Learning Curves)\n",
    "# ------------------------------------------------------\n",
    "print(\"\\nüìà Generating learning curves (Bias‚ÄìVariance Analysis)...\")\n",
    "\n",
    "def plot_learning_curve(model, X, y, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=3, scoring='r2', n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color='blue', label='Train R¬≤')\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color='green', label='CV R¬≤')\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"R¬≤ Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"images/{title}.png\")\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "plot_learning_curve(sgd, X_train_capped, y_train, title=\"Learning Curve ‚Äì SGD Regressor\")\n",
    "plot_learning_curve(best_svr, X_train_svr, y_train_svr, title=\"Learning Curve ‚Äì Tuned SVR (Subsampled)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14) SVR Margin / Maximum Margin Concept\n",
    "\n",
    "# %% [code]\n",
    "print(\"\\nüìä Visualizing SVR Margin (Epsilon-Tube)...\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_train_svr['MedInc'], y_train_svr, alpha=0.3, label='Training Points')\n",
    "\n",
    "# Create 500 evenly spaced values of MedInc\n",
    "X_plot = np.linspace(X_train_svr['MedInc'].min(), X_train_svr['MedInc'].max(), 500)\n",
    "\n",
    "# Create DataFrame with same columns as training, other features filled with median\n",
    "X_plot_df = pd.DataFrame(np.tile(X_train_svr.median().values, (500, 1)), columns=X_train_svr.columns)\n",
    "X_plot_df['MedInc'] = X_plot\n",
    "\n",
    "# Predict using SVR\n",
    "y_svr_plot = best_svr.predict(X_plot_df)\n",
    "\n",
    "plt.plot(X_plot, y_svr_plot, color='red', label='SVR Prediction')\n",
    "plt.fill_between(\n",
    "    X_plot,\n",
    "    y_svr_plot - best_svr.epsilon,\n",
    "    y_svr_plot + best_svr.epsilon,\n",
    "    color='orange', alpha=0.3, label='Epsilon Tube (Margin)'\n",
    ")\n",
    "plt.xlabel(\"Median Income (Scaled)\")\n",
    "plt.ylabel(\"Median House Value\")\n",
    "plt.title(\"SVR Epsilon Tube ‚Äì Maximum Margin Concept (Subsampled)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/SVR-Episilon Tube_Maximum_margin_concept.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "\n",
    "# =========================\n",
    "# 15) Model Comparison & Dynamic Summary (Professional)\n",
    "# =========================\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "print(\"\\nüìä Generating full model comparison with professional visuals...\")\n",
    "\n",
    "# Recalculate predictions for key models\n",
    "y_pred_best_svr_full = best_svr.predict(X_test_capped)\n",
    "y_pred_best_rf = best_rf.predict(X_test_capped)\n",
    "\n",
    "# Prepare comparison data (exclude SGD initially)\n",
    "comparison_data_main = [\n",
    "    [\"Linear Regression\", mean_squared_error(y_test, models[\"Linear Regression\"].predict(X_test_capped)),\n",
    "     mean_absolute_error(y_test, models[\"Linear Regression\"].predict(X_test_capped)),\n",
    "     r2_score(y_test, models[\"Linear Regression\"].predict(X_test_capped))],\n",
    "    \n",
    "    [\"Ridge Regression\", mean_squared_error(y_test, models[\"Ridge Regression\"].predict(X_test_capped)),\n",
    "     mean_absolute_error(y_test, models[\"Ridge Regression\"].predict(X_test_capped)),\n",
    "     r2_score(y_test, models[\"Ridge Regression\"].predict(X_test_capped))],\n",
    "    \n",
    "    [\"Decision Tree\", mean_squared_error(y_test, models[\"Decision Tree\"].predict(X_test_capped)),\n",
    "     mean_absolute_error(y_test, models[\"Decision Tree\"].predict(X_test_capped)),\n",
    "     r2_score(y_test, models[\"Decision Tree\"].predict(X_test_capped))],\n",
    "    \n",
    "    [\"Random Forest (Tuned)\", mean_squared_error(y_test, y_pred_best_rf),\n",
    "     mean_absolute_error(y_test, y_pred_best_rf),\n",
    "     r2_score(y_test, y_pred_best_rf)],\n",
    "    \n",
    "    [\"Gradient Boosting\", mean_squared_error(y_test, models[\"Gradient Boosting\"].predict(X_test_capped)),\n",
    "     mean_absolute_error(y_test, models[\"Gradient Boosting\"].predict(X_test_capped)),\n",
    "     r2_score(y_test, models[\"Gradient Boosting\"].predict(X_test_capped))],\n",
    "    \n",
    "    [\"KNN Regressor\", mean_squared_error(y_test, models[\"KNN Regressor\"].predict(X_test_capped)),\n",
    "     mean_absolute_error(y_test, models[\"KNN Regressor\"].predict(X_test_capped)),\n",
    "     r2_score(y_test, models[\"KNN Regressor\"].predict(X_test_capped))],\n",
    "    \n",
    "    [\"Tuned SVR\", mean_squared_error(y_test, y_pred_best_svr_full),\n",
    "     mean_absolute_error(y_test, y_pred_best_svr_full),\n",
    "     r2_score(y_test, y_pred_best_svr_full)]\n",
    "]\n",
    "\n",
    "# SGD Regressor (extreme scale)\n",
    "comparison_sgd = [\n",
    "    [\"SGD Regressor\", mean_squared_error(y_test, y_pred_sgd),\n",
    "     mean_absolute_error(y_test, y_pred_sgd),\n",
    "     r2_score(y_test, y_pred_sgd)]\n",
    "]\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_main = pd.DataFrame(comparison_data_main, columns=['Model','MSE','MAE','R2']).sort_values('R2', ascending=False).reset_index(drop=True)\n",
    "df_sgd = pd.DataFrame(comparison_sgd, columns=['Model','MSE','MAE','R2'])\n",
    "\n",
    "all_results_full_sorted = pd.concat([df_main, df_sgd]).sort_values('R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Display comparison table\n",
    "# ----------------------------\n",
    "print(\"üìä Full Model Comparison Table:\")\n",
    "display(all_results_full_sorted)\n",
    "\n",
    "# Normalize metrics for visualization (excluding SGD)\n",
    "df_main_norm = df_main.copy()\n",
    "df_main_norm['MSE_norm'] = df_main_norm['MSE'] / df_main_norm['MSE'].max()\n",
    "df_main_norm['MAE_norm'] = df_main_norm['MAE'] / df_main_norm['MAE'].max()\n",
    "df_main_norm['R2_norm'] = df_main_norm['R2']  # R2 already 0‚Äì1\n",
    "\n",
    "# ----------------------------\n",
    "# Professional custom palette\n",
    "# ----------------------------\n",
    "custom_palette = list(mcolors.TABLEAU_COLORS.values())[:len(df_main_norm)]\n",
    "\n",
    "# ----------------------------\n",
    "# Horizontal bar plots\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='R2_norm', y='Model', hue='Model',data=df_main_norm, palette=custom_palette)\n",
    "plt.title(\"Model Comparison ‚Äì R¬≤ Score (Normalized)\")\n",
    "plt.xlabel(\"R¬≤ Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/Model Comparison - Normalized - R2 score.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='MAE_norm', y='Model',hue='Model', data=df_main_norm, palette=custom_palette)\n",
    "plt.title(\"Model Comparison ‚Äì MAE (Normalized)\")\n",
    "plt.xlabel(\"Normalized MAE\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/Model Comparison - Normalized - MAE.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='MSE_norm', y='Model',hue='Model', data=df_main_norm, palette=custom_palette)\n",
    "plt.title(\"Model Comparison ‚Äì MSE (Normalized)\")\n",
    "plt.xlabel(\"Normalized MSE\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/Model Comparison - Normalized - MSE.png\")\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Display SGD separately\n",
    "# ----------------------------\n",
    "print(\"\\n‚ö†Ô∏è SGD Regressor is extremely unstable; metrics are on a very different scale:\")\n",
    "display(df_sgd)\n",
    "\n",
    "# ----------------------------\n",
    "# Dynamic Summary\n",
    "# ----------------------------\n",
    "best_r2_model = df_main.loc[df_main['R2'].idxmax()]['Model']\n",
    "best_r2 = df_main['R2'].max()\n",
    "\n",
    "best_mae_model = df_main.loc[df_main['MAE'].idxmin()]['Model']\n",
    "best_mae = df_main['MAE'].min()\n",
    "\n",
    "best_mse_model = df_main.loc[df_main['MSE'].idxmin()]['Model']\n",
    "best_mse = df_main['MSE'].min()\n",
    "\n",
    "print(\"üìå Dynamic Summary & Recommendation:\")\n",
    "print(f\"- The top-performing model based on R¬≤ is **{best_r2_model}** (R¬≤ = {best_r2:.3f}).\")\n",
    "print(f\"- Lowest MAE: **{best_mae_model}** (MAE = {best_mae:.3f}), indicating best average prediction accuracy.\")\n",
    "print(f\"- Lowest MSE: **{best_mse_model}** (MSE = {best_mse:.3f}), indicating best overall error minimization.\")\n",
    "print(\"- Random Forest and Gradient Boosting are strong classical models; Tuned SVR provides a competitive non-linear alternative.\")\n",
    "print(\"- SGD Regressor is not recommended due to extreme instability and poor performance.\")\n",
    "\n",
    "#formatted_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#print(\"Execution completed at\", formatted_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345079a2-89cd-4bff-b9be-9bbfec74d130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
